scenario_evaluator_task:
  description: >
    AI agents often have to interact with tools to perform tasks. It is important to test that tools are being called when a user query expects a certain tool to be called. It's also important to know when a tool should not be called.

    Based on the provided tools: {tools}

    Create a comprehensive list of scenarios that should be tested to verify that the AI agent works successfully. For each scenario, include:
    1. A clear scenario name
    2. A detailed description of what the user is asking
    3. An explanation of why this scenario is suitable for testing
    4. The expected_tool_call field should contain just the tool name (e.g., "get_weather") or None if no tool should be called

    Consider various types of scenarios:
    - Positive cases where tools should be called with correct parameters
    - Negative cases where no tool should be called
    - Edge cases with missing or invalid parameters
    - Ambiguous queries that might be interpreted differently
    - Multiple tool scenarios if applicable

  expected_output: >
    A list of scenarios that should be tested to verify that the AI agent works successfully. Each scenario should be a structured object with name, description, why_its_suitable, and expected_tool_call fields. The expected_tool_call field should contain only the tool name (not the full function call with parameters) or None.
